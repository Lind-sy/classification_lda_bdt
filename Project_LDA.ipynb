{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import all libraries that we will later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Split ratio - defines proportion how should we split test data and training data \n",
    " In this case 90% training data and 10 % test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First create constructor for our LDA Classifier \n",
    " data - our given data set \n",
    " descriptor (dimensions)- this value defines how many eigen vectors we would choose to do the data sample projection \n",
    " label_column - is data set column containing class labels for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    def __init__(self, data, descriptors=1, label_column=-1):\n",
    "        self.data = data\n",
    "        self.descriptors = descriptors\n",
    "        self.label_column = label_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Method is used for excluding label column from data set is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def remove_label_column(self, data, col):\n",
    "        return data.drop(data.columns[[col]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What we need is to do LDA parameter estimation \n",
    " Procedure is like this:\n",
    "     First we calculate means for each class\n",
    "     Than we calculate the overall mean of all the data\n",
    "     calculate between class covariance matrix - S_B = {N_i (m_i - m) (m_i - m).T}\n",
    "     calculate within class covariance matrix - S_W = {S_i}; S_i = {(x - m_i) (x - m_i).T}\n",
    "     What we do now is find eigenvalue, eigenvector pairs for inv(S_W).S_B\n",
    "      sort the eigvals in decreasing order\n",
    "      take first eigvectors that descrbe the biggest vairance in data, we choose to take first 3 becaause descriptor in our case is defined as 3\n",
    "      return the LDA parameters - mean values and choosen eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def estimate_lda_params(self, data):\n",
    "        means = {}\n",
    "        for c in self.classes:\n",
    "            # calculate for columns\n",
    "            tmp_data_mean = self.remove_label_column(self.class_data_samples[c], self.label_column).mean(axis=0)\n",
    "            means[c] = np.array(tmp_data_mean)\n",
    "\n",
    "        overall_mean = np.array(self.remove_label_column(data, self.label_column).mean(axis=0))\n",
    "\n",
    "        # calculate between class covariance matrix\n",
    "        S_B = np.zeros((data.shape[1] - 1, data.shape[1] - 1))\n",
    "        for c in means.keys():\n",
    "            N = len(self.class_data_samples[c])\n",
    "            scatter_mean = np.outer((means[c] - overall_mean), (means[c] - overall_mean))\n",
    "            S_B += np.multiply(N, scatter_mean)\n",
    "\n",
    "        # calculate within class covariance matrix\n",
    "        S_W = np.zeros(S_B.shape)\n",
    "        for c in self.classes:\n",
    "            class_data_transpose = self.remove_label_column(self.class_data_samples[c], self.label_column).T\n",
    "            tmp_data = np.subtract(class_data_transpose, np.expand_dims(means[c], axis=1))  # taking away mean value\n",
    "            cov_mat = np.cov(tmp_data)\n",
    "            S_W = np.add(cov_mat, S_W)\n",
    "\n",
    "        invSw_bySb = np.dot(np.linalg.pinv(S_W), S_B)\n",
    "        eigvals, eigvecs = np.linalg.eig(invSw_bySb)\n",
    "        eiglist = [(eigvals[i], eigvecs[:, i]) for i in range(len(eigvals))]\n",
    "\n",
    "        # sort the eigvals in decreasing order\n",
    "        eiglist = sorted(eiglist, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # take the first descriptors eigvectors\n",
    "        w = np.array([eiglist[i][1] for i in range(self.descriptors)])\n",
    "\n",
    "        self.w = w\n",
    "        self.means = means\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we define fit function \n",
    " First group data based on the classes \n",
    " Now we get how many unique classes are in our data set\n",
    " Next we go trough all the classes\n",
    "     Now we get current class group with all it's sample data out of all groups\n",
    "     we find indexes of each class sample data in the group \n",
    "     splitting bordder defines how we will divide class samples. \n",
    "     Which samples will go to test and which will go to training data.\n",
    "     After the splitting is done we concatine data samples of each class together \n",
    "     Last step is to pass our training data to method that estimates LDA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def fit(self):\n",
    "        traindata = []\n",
    "        testdata = []\n",
    "        grouped = self.data.groupby(self.data.ix[:, self.label_column])\n",
    "\n",
    "        self.classes = [c for c in grouped.groups.keys()]\n",
    "        self.class_data_samples = {}\n",
    "        for c in self.classes:\n",
    "            self.class_data_samples[c] = grouped.get_group(c)\n",
    "            index_list = list(self.class_data_samples[c].index)\n",
    "            splitting_border = int(self.class_data_samples[c].shape[0] * split_ratio)\n",
    "            rows = random.sample(index_list, splitting_border)\n",
    "            traindata.append(self.class_data_samples[c].ix[rows])\n",
    "            testdata.append(self.class_data_samples[c].drop(rows))\n",
    "\n",
    "        traindata = pd.concat(traindata)\n",
    "        testdata = pd.concat(testdata)\n",
    "\n",
    "        # estimate the LDA parameters\n",
    "        self.estimate_lda_params(traindata)\n",
    "\n",
    "        return traindata, testdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Second phase is to create gaussian model\n",
    " Create method to estimate gaussian models for each class.\n",
    " Estimates priors, means and covariances for each class.\n",
    "     We drop the label column for each class sample data \n",
    "     Next we project the data samples into the d dimension where d are the choosen dimensions in our case it is 3 \n",
    "        Descriptor vectors (eigenvectors) are multiplied by Transposed class samples and than the result is again transposed\n",
    "     Need to calculate projected data mean value, covariance matrix and priors for multi-variate Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def gaussian_modeling(self):\n",
    "        self.priors = {}\n",
    "        self.gaussian_means = {}\n",
    "        self.gaussian_cov = {}\n",
    "\n",
    "        for c in self.means.keys():\n",
    "            input_data = self.remove_label_column(self.class_data_samples[c], self.label_column)\n",
    "            projection = np.dot(self.w, input_data.T).T\n",
    "            self.priors[c] = input_data.shape[0] / float(self.data.shape[0])\n",
    "            self.gaussian_means[c] = np.mean(projection, axis=0)\n",
    "            self.gaussian_cov[c] = np.cov(projection, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methos that return the probability density function result for gaussian, given an samole projection point, gaussian mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def pdf(self, sample, mean, cov):\n",
    "        cons = 1. / ((2 * np.pi) ** (len(sample) / 2.) * np.linalg.det(cov) ** (-0.5))\n",
    "        exponent = np.exp(-np.dot(np.dot((sample - mean), np.linalg.inv(cov)), (sample - mean).T) / 2.)\n",
    "        return cons * exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next step is to calculate Gaussian score or in other words maximum likelihood\n",
    " First we sort our data by classes\n",
    " Remove label column\n",
    " Calculate the likelihoods for each class based on our created gaussian models for this data set\n",
    " Maximum likelihood is calculated like this:\n",
    "     Multiply class prior value with gausian probabilty density function for each projected sample point\n",
    " When that is done we choose sample points class label taking the biggest likelihood score\n",
    " After we calculate the error between actual class labels and expected class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def calculate_score_gaussian(self, data):\n",
    "        classes = sorted(list(self.means.keys()))\n",
    "        input_data = self.remove_label_column(data, self.label_column)\n",
    "        projection = np.dot(self.w, input_data.T).T\n",
    "        # calculate the likelihoods for each class based on the gaussian models\n",
    "        likelihoods = np.array([\n",
    "            [self.priors[c] * self.pdf(\n",
    "                [x[index] for index in range(len(x))],\n",
    "                self.gaussian_means[c],\n",
    "                self.gaussian_cov[c])\n",
    "             for c in classes]\n",
    "            for x in projection])\n",
    "        labels = np.argmax(likelihoods, axis=1)\n",
    "        errors = np.sum(labels != data.ix[:, self.label_column])\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we can visualize projections for the entire data in a 2D plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def plot_proj_2D(self, data):\n",
    "        classes = list(self.means.keys())\n",
    "        color_map = cm.rainbow(np.linspace(0, 1, len(classes)))\n",
    "        plotlabels = {classes[c]: color_map[c] for c in range(len(classes))}\n",
    "\n",
    "        fig = plt.figure()\n",
    "        for i, row in data.iterrows():\n",
    "            projection = np.dot(self.w, row[:self.label_column])\n",
    "            projection = np.real(projection)\n",
    "            plt.scatter(projection[0], projection[1], color=plotlabels[row[self.label_column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now can do the same  but this time in a 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(LDA):    \n",
    "    def plot_proj_3D(self, data):\n",
    "        classes = list(self.means.keys())\n",
    "        color_map = cm.rainbow(np.linspace(0, 1, len(classes)))\n",
    "        plotlabels = {classes[c]: color_map[c] for c in range(len(classes))}\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        for i, row in data.iterrows():\n",
    "            projection = np.dot(self.w, row[:self.label_column])\n",
    "            projection = np.real(projection)\n",
    "            ax.scatter(projection[0],projection[1],projection[2],color=plotlabels[row[self.label_column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In main method \n",
    " We read or data set of digits \n",
    " Create LDA classifier with 3 descriptors (dimensions)\n",
    " What we do next is estimate LDA Parameter's and return training and test data set\n",
    " Create gaussian model \n",
    " Calculate test and training error\n",
    " Display data projection in 3D and 2D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('digits.csv')\n",
    "    data = data[data['0.29'] <= 4]\n",
    "    lda = LDA(data, descriptors=3)\n",
    "    traindata, testdata = lda.fit()\n",
    "\n",
    "    lda.gaussian_modeling()\n",
    "    trainerror = lda.calculate_score_gaussian(traindata) / float(traindata.shape[0])\n",
    "    testerror = lda.calculate_score_gaussian(testdata) / float(testdata.shape[0])\n",
    "    print('Training error:', trainerror)\n",
    "    print('Test error', testerror)\n",
    "    lda.plot_proj_3D(data)\n",
    "    lda.plot_proj_2D(data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
